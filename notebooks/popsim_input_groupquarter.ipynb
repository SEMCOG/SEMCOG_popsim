{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate control totals needed by populationsim(RSG)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from census import Census\n",
    "import yaml\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = yaml.load(open(\"./region_gq.yml\", \"r\"), Loader=yaml.Loader)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"b01f26cd56a7f5457534e8436a1e63f4c7189b23\", year=2010).sf1\n",
    "prj_name = conf[\"region\"][\"name\"]\n",
    "state = conf[\"region\"][\"state\"][0]\n",
    "counties = conf[\"region\"][\"counties\"]\n",
    "pre_folder = conf[\"preprocess\"][\"folder\"]\n",
    "gq_buildings = conf[\"preprocess\"][\"gq_buildings\"]\n",
    "h_pums_csv = \"../\" + pre_folder + conf[\"preprocess\"][\"h_pums_csv\"]\n",
    "p_pums_csv = \"../\" + pre_folder + conf[\"preprocess\"][\"p_pums_csv\"]\n",
    "\n",
    "ouptut_geo_cross = \"../{}{}_geo_cross_walk.csv\".format(pre_folder, prj_name)\n",
    "output_control = \"../{}{}_control_totals_.csv\".format(pre_folder, prj_name)\n",
    "output_seed_hhs = \"../{}{}_seed_households.csv\".format(pre_folder, prj_name)\n",
    "output_seed_persons = \"../{}{}_seed_persons.csv\".format(pre_folder, prj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class census_downloader:\n",
    "\n",
    "    def __init__(self, census_reader, states, counties = None, tract_ids = None, blockgroup_ids = None):\n",
    "        self.states = states\n",
    "        self.counties = counties\n",
    "        self.tracts = tract_ids\n",
    "        self.blockgroups = blockgroup_ids \n",
    "        self.cread = census_reader\n",
    "        self.udpate_states_counties([self.states, self.counties])\n",
    "            \n",
    "    def state_download(self, vars):\n",
    "        return self.cread.get(vars, geo={'for': 'state:{}'.format(self.states)})\n",
    "\n",
    "    def county_download(self, vars):\n",
    "        print (self.states,self.counties)\n",
    "        print(vars)\n",
    "        return self.cread.get(vars, geo={'for': 'county:{}'.format(self.counties), \n",
    "                                        'in': 'state:{}'.format(self.states)})\n",
    "\n",
    "    def tract_download(self, vars):\n",
    "        return self.cread.get(vars, geo={'for': 'tract:{}'.format(self.tracts), \n",
    "                                    'in': 'state:{} county:{}'.format(self.states, self.counties)})\n",
    "\n",
    "    def blockgroup_download(self, vars):\n",
    "        clst = self.counties.split(',')\n",
    "        cm = []\n",
    "        for cn in clst:\n",
    "            cm += self.cread.get(vars, geo={'for': 'block group:{}'.format(self.blockgroups), \n",
    "                                'in': 'state:{} county:{} tract:{}'.format(self.states, cn,                                                                                     self.tracts)}) \n",
    "        return cm\n",
    "    \n",
    "    def fips_lookup(self, states, counties = None):\n",
    "        if counties == \"*\": counties = None\n",
    "        fips_table = pd.read_csv(\n",
    "                \"https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt\",\n",
    "                header=None, names=['state','state_fips', 'county_fips', 'county' ,'type'],                          dtype=str)  \n",
    "        qstr = '(state in {})'.format(states)\n",
    "        if counties:\n",
    "            qstr += ' & (county in {})'.format(counties)\n",
    "            dfq = fips_table.query(qstr)\n",
    "            return list(dfq.state_fips.unique()), list(dfq.county_fips.unique())\n",
    "        dfq = fips_table.query(qstr)\n",
    "        print(list(dfq.state_fips.unique()))\n",
    "        return list(dfq.state_fips.unique()), None\n",
    "\n",
    "    def udpate_states_counties(self, geos):\n",
    "        for i in [0,1]:\n",
    "            if geos[i] != None:\n",
    "                if (type(geos[i]) != list) & (geos[i] != '*'):\n",
    "                    geos[i] = [geos[i]]\n",
    "                geos[i] = [str(x) for x in geos[i]]\n",
    "        if (type(geos[0]) == list) & (geos[0][0].isdigit() == False):\n",
    "            geos[0], geos[1] = self.fips_lookup(geos[0], geos[1])\n",
    "        for i in [0,1]:\n",
    "            if geos[i]: geos[i] = ','.join([str(x).zfill(i+2) for x in geos[i]]) \n",
    "                        #i+2 cause state and county need 2 and 3 0s in lead\n",
    "        self.states = geos[0]\n",
    "        self.counties = geos[1]\n",
    "\n",
    "\n",
    "    def download(self, variables):\n",
    "        dfm = pd.DataFrame()\n",
    "        if not(self.counties):\n",
    "            downv = self.state_download(variables)\n",
    "        elif not(self.tracts):\n",
    "            downv = self.county_download(variables)\n",
    "        elif not(self.blockgroups):\n",
    "            downv = self.tract_download(variables)\n",
    "        else:\n",
    "            downv = self.blockgroup_download(variables)\n",
    "        dfm = pd.DataFrame.from_dict(downv)\n",
    "\n",
    "        return dfm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make GQ cross walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm  = pd.HDFStore(\"../\" + pre_folder + conf[\"preprocess\"][\"model_hdf\"], 'r')\n",
    "\n",
    "blgs = pd.merge(stm['buildings'][['parcel_id','b_zone_id','b_city_id']], \n",
    "                stm['parcels'][['semmcd', 'county_id','census_bg_id']], \n",
    "                left_on='parcel_id', right_index=True, how='left')\n",
    "blgs = blgs.reset_index()\n",
    "\n",
    "blgs.rename(columns = {'b_zone_id':'ZONE','b_city_id':'B_CITY_ID', \n",
    "                        'census_bg_id':'BLKGRPCE', 'parcel_id':'PARCEL', \n",
    "                        'semmcd': 'MCD', 'county_id':'COUNTYFP', \n",
    "                        'building_id':'BUILDING_ID'}, inplace=True)\n",
    "blgs = blgs.fillna(0)\n",
    "blgs = blgs.astype(int)\n",
    "blgs['STATEFP'] = '26'\n",
    "blgs['COUNTYFP'] = blgs['COUNTYFP'].astype(str).str.zfill(3)\n",
    "blgs['BLKGRPCE'] = blgs['BLKGRPCE'].astype(str).str.zfill(7)\n",
    "blgs['TRACTCE'] = blgs['BLKGRPCE'].str[:-1]\n",
    "blgs['REGION'] = '1'\n",
    "blgs['BLKGRPID'] = blgs['STATEFP'] + blgs['COUNTYFP'] + blgs['BLKGRPCE'] \n",
    "blgs['TRACTID'] = blgs['STATEFP'] + blgs['COUNTYFP'] + blgs['TRACTCE'] \n",
    "\n",
    "df_tract_puma = pd.read_csv(conf[\"geographies\"][\"tract_puma_file\"], dtype=str)\n",
    "\n",
    "GQ_cross_walk = pd.merge(blgs, df_tract_puma, on=['STATEFP', 'COUNTYFP', 'TRACTCE'], how='left')\n",
    "GQ_cross_walk.rename(columns={'PUMA5CE':'PUMA'}, inplace = True)\n",
    "# GQ_cross_walk[['BUILDING_ID', 'COUNTYFP', 'ZONE', 'BLKGRPCE', 'PUMA','REGION','BLKGRPID', 'TRACTID']].set_index('BUILDING_ID').to_csv(ouptut_geo_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blgs.loc[blgs.BLKGRPID==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building controls\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gq_buildings = pd.read_csv(\"../\" + pre_folder + conf[\"preprocess\"][\"gq_buildings\"])\n",
    "bt = 0\n",
    "for x in range(100, 700, 100):\n",
    "    bt += 1\n",
    "    gq_buildings.loc[(gq_buildings.GQ_CODE>=x) & (gq_buildings.GQ_CODE<(x+100)), \n",
    "                            'type'] = 'GQTYPE' +str(bt) + 'B'\n",
    "gq_buildings.loc[(gq_buildings.GQ_CODE>=700), 'type'] = 'GQTYPE7B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldgctrl = pd.crosstab(gq_buildings.BUILDING_ID, gq_buildings['type'],\n",
    "                        values=gq_buildings.RESIDENT_COUNT, aggfunc='sum' )\n",
    "bldgctrl.fillna(0, inplace=True)\n",
    "bldgctrl['HHBASE'] = bldgctrl.sum(axis = 1, skipna = True)\n",
    "bldgctrl = bldgctrl.reset_index()\n",
    "bldgctrl = pd.merge(bldgctrl, GQ_cross_walk[['BUILDING_ID','BLKGRPID', 'REGION','PUMA']], \n",
    "                        on ='BUILDING_ID',how='left')\n",
    "bldgctrl = bldgctrl.loc[bldgctrl.PUMA.notnull()]\n",
    "bldgctrl.to_csv(output_control.replace(\".csv\", \"building.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLKGRPlist = bldgctrl.BLKGRPID.unique()\n",
    "GQ_cross_walk = GQ_cross_walk.loc[GQ_cross_walk.BLKGRPID.isin(BLKGRPlist)]\n",
    "GQ_cross_walk[['BUILDING_ID', 'COUNTYFP', 'ZONE', 'BLKGRPCE', 'PUMA','REGION','BLKGRPID', 'TRACTID']].set_index('BUILDING_ID').to_csv(ouptut_geo_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Census block group controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list Census marginal variables from controls_pre table (same as \"controls\" table with additional \"acs_variables\" field )\n",
    "# \"acs_variables\" field contains evaluation expressions, including variables from Census API and operations\n",
    "dfc = pd.read_csv(\"../\" + pre_folder + conf[\"preprocess\"][\"pre_control\"])\n",
    "dfc = dfc.loc[dfc.acs_variables.notnull()]\n",
    "\n",
    "dic_margs = {}\n",
    "for geo, dfgeo in dfc.groupby('geography'):  \n",
    "    full_vars= list(set(re.findall(r'P[0-9]{3}[A-Z]{0,1}[0-9]{3}', str(list(dfgeo.acs_variables)))))\n",
    "    if geo == 'BLKGRP':\n",
    "        ac5 = census_downloader(c, state, counties, \"*\", \"*\")\n",
    "        geo_cols = ['state', 'county', 'tract', 'block group']\n",
    "    elif geo == 'TRACT':\n",
    "        ac5 = census_downloader(c, state, counties, \"*\")\n",
    "        geo_cols = ['state', 'county', 'tract']\n",
    "    \n",
    "    if full_vars != []:\n",
    "        dic_margs[geo] = ac5.download(full_vars).set_index(geo_cols)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile Census marginals to popsim control variables\n",
    "for geo, dfg in dfc.groupby('geography'):\n",
    "    for ind, r in dfg.iterrows():\n",
    "        if True:\n",
    "            print(r['acs_variables'])\n",
    "            dic_margs[geo] = dic_margs[geo].astype(float).fillna(0)\n",
    "            dic_margs[geo][r['control_field']] = dic_margs[geo].eval(r['acs_variables'].replace('\"', ''))\n",
    "    if geo in dic_margs.keys():\n",
    "        dic_margs[geo] = dic_margs[geo][list(dfg.control_field)] #keep only control fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unique geoids and PUMA \n",
    "for geo, dfm in dic_margs.items():\n",
    "\n",
    "    dfcross = pd.read_csv(ouptut_geo_cross, dtype = str)\n",
    "    if dfm.index.nlevels == 3:\n",
    "        dfm['TRACTID'] = ['{}{}{}'.format(l1.zfill(2),l2.zfill(3),l3.zfill(6)) \n",
    "                                                                for l1, l2, l3 in dfm.index]\n",
    "        dfcr = dfcross.drop_duplicates('TRACTID')\n",
    "        dfm = pd.merge(dfm.reset_index(), dfcr[['TRACTID','PUMA']], on = 'TRACTID', how = 'left')\n",
    "    elif dfm.index.nlevels == 4:\n",
    "        dfcr = dfcross.drop_duplicates('BLKGRPID')\n",
    "        dfm['BLKGRPID'] = ['{}{}{}{}'.format(l1.zfill(2),l2.zfill(3),l3.zfill(6), l4) \n",
    "                                                                for l1, l2, l3, l4 in dfm.index]\n",
    "        dfm = pd.merge(dfm.reset_index(), dfcr[['BLKGRPID','PUMA']], on = 'BLKGRPID',how = 'left')\n",
    "    dfm.columns = [col.upper() for col in dfm.columns]\n",
    "    dfm = dfm.loc[dfm.BLKGRPID.isin(BLKGRPlist)]\n",
    "    dfm.to_csv(output_control.replace(\".csv\", geo.lower() + \".csv\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.drop(['GQTYPE1','GQTYPE2','GQTYPE3','GQTYPE4','GQTYPE5','GQTYPE6','GQTYPE7'], axis=1).to_csv(output_control.replace(\".csv\", geo.lower() + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.to_csv(output_control.replace(\".csv\", geo.lower() + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AJDUST Census marginals"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gq = {\n",
    "    \"INST\":\n",
    "        {'TYPE':['GQTYPE1', 'GQTYPE2', 'GQTYPE3', 'GQTYPE4'],\n",
    "        'POP':'INSTPOP',\n",
    "        'BPOP':'INSTPOP2'\n",
    "        },\n",
    "    \"NONI\":\n",
    "        {'TYPE':['GQTYPE5', 'GQTYPE7'],\n",
    "        'POP':'NONIPOP',\n",
    "        'BPOP':'NONIPOP2'\n",
    "        }\n",
    "    }\n",
    "dict_gq[\"INST\"]['BTYPE'] = [x + 'B'  for x in dict_gq[\"INST\"]['TYPE']]\n",
    "dict_gq[\"NONI\"]['BTYPE'] = [x + 'B'  for x in dict_gq[\"NONI\"]['TYPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2 = dfm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gq['INST']['VARS'] =[x for x in dfmsel2.columns if \"INST\" in x][1:]\n",
    "dict_gq['NONI']['VARS'] =[x for x in dfmsel2.columns if \"NONI\" in x][1:]\n",
    "print(dict_gq['INST']['VARS'] , dict_gq['NONI']['VARS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_blk  = bldgctrl.groupby('BLKGRPID')[dict_gq[\"INST\"]['BTYPE'] + dict_gq[\"NONI\"]['BTYPE']].agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2 = pd.merge(dfmsel2, bldg_blk, left_on ='BLKGRPID', right_index=True, how = 'left')\n",
    "for k in dict_gq.keys():\n",
    "    dfmsel2[dict_gq[k]['BPOP']] = dfmsel2[dict_gq[k]['BTYPE']].sum(axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Census variable(age, race, sex) mean cross block groups by single GQTYPE (only one GQTYPE for BGs)\n",
    "dic_mean = {}\n",
    "for k in dict_gq.keys():\n",
    "    print(k)\n",
    "    for col in dict_gq[k]['TYPE']:\n",
    "        print(col)\n",
    "        selind = dfmsel2.loc[(dfmsel2[col]>0) &(dfmsel2[col]==dfmsel2[dict_gq[k]['POP']])].index\n",
    "        dic_mean[col+'B'] = dfmsel2.loc[selind, dict_gq[k]['VARS']].div(dfmsel2.loc[selind, dict_gq[k]['POP']], axis=0).mean()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign above means to 0 pop marginal rows by building GQTYPE\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_gq.keys():\n",
    "    print(k)\n",
    "    zeroindex = dfmsel2.loc[dfmsel2[dict_gq[k]['POP']]==0].index\n",
    "    for col in dict_gq[k]['BTYPE']:\n",
    "        dfind = dfmsel2.loc[dfmsel2.index.isin(zeroindex) & (dfmsel2[col]>0)]\n",
    "        dfmsel2.loc[dfind.index, dict_gq[k]['VARS']] += np.repeat([dic_mean[col]], len(dfind), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_gq.keys():\n",
    "    for n in ['AGE', 'RACE', 'SEX']:\n",
    "        selc = [c for c in dfmsel2.columns if (k in c) & (n in c)]\n",
    "        dfmsel2[selc] = dfmsel2[selc].div(dfmsel2[selc].sum(axis=1), axis=0)\n",
    "        dfmsel2.fillna(0, inplace = True)\n",
    "        dfmsel2[selc] = dfmsel2[selc].multiply(dfmsel2[dict_gq[k]['BPOP']], axis=0)\n",
    "    dfmsel2[dict_gq[k]['POP']] = dfmsel2[dict_gq[k]['BPOP']]\n",
    "\n",
    "    for t, bt in zip(dict_gq[k]['TYPE'],dict_gq[k]['BTYPE']):\n",
    "        dfmsel2[t] = dfmsel2[bt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in dfmsel2.columns if ('GQ' in c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2.drop([c for c in dfmsel2.columns if ('GQ' in c) | ('POP2' in c)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2.drop([c for c in dfmsel2.columns if ('GQ' in c) | ('POP2' in c)], axis =1).to_csv(output_control.replace(\".csv\", geo.lower() + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmsel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GQ seed HHs and persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_lst = GQ_cross_walk.PUMA.fillna(0).astype(int).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gq_p_pums = pd.read_csv(p_pums_csv)\n",
    "gq_p_pums = gq_p_pums.loc[gq_p_pums.PUMA.isin(puma_lst)]\n",
    "gq_p_pums = gq_p_pums.loc[gq_p_pums[\"RELP\"].isin([16,17])]\n",
    "gq_p_pums['hh_id'] = gq_p_pums['SERIALNO']\n",
    "gq_p_pums.to_csv(output_seed_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gq_h_pums =  pd.read_csv(h_pums_csv, index_col=\"SERIALNO\")\n",
    "gq_h_pums = gq_h_pums.loc[gq_h_pums.PUMA.isin(puma_lst)]\n",
    "gq_h_pums = gq_h_pums.loc[(gq_h_pums.TYPE > 1) & (gq_h_pums.NP > 0 )]\n",
    "gq_h_pums['hh_id'] =  gq_h_pums.index.values\n",
    "gq_h_pums = pd.merge(gq_h_pums, gq_p_pums[['SERIALNO', 'PWGTP']], on = 'SERIALNO', how='left')\n",
    "gq_h_pums['WGTP'] = gq_h_pums['PWGTP'] \n",
    "gq_h_pums.to_csv(output_seed_hhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = dic_margs['BLKGRP'].copy()\n",
    "# for k in ['POP', 'RACE', 'AGE', 'SEX']:\n",
    "#     for c in ['INST', 'NONI']:\n",
    "#         cols= []\n",
    "#         for col in cm.columns:\n",
    "#             if (k in col) & (c in col):\n",
    "#                 cols.append(col)\n",
    "#         print(cols)\n",
    "#         print('target', target2015[c], 'before',cm[cols].sum().sum(), )\n",
    "#         cm[cols] = round((cm[cols]/(cm[cols].sum().sum()) * target2015[c]), 0)\n",
    "#         print('after', cm[cols].sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = pd.HDFStore(\"../output/pipeline.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda6f9e6f4ac8a84988b167991f607929ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}