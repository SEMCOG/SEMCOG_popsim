{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda6f9e6f4ac8a84988b167991f607929ce",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refinement test\n",
    "\n",
    "#input development\n",
    "#1. geo cross walk table:\n",
    "    #specify refine target geo: city, taz, blkgrp, parcels\n",
    "    #specify sample unit: city, taz, blkgrp, parcels\n",
    "    #settings: sample geography = \"SAMPLEGEO\"\n",
    "\n",
    "#2. control table\n",
    "    #convert from control totals: \n",
    "        # read categorical info and fill control table\n",
    "        # summarize inputs by categories to use as baseline\n",
    "    #new controls: \n",
    "        #a. manual modify specific controls \n",
    "        #b. read target database and make controls by category\n",
    "\n",
    "#3. seed tables\n",
    "    #base inputs with sample and refine geo units\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_geos(st, year, weight = None):\n",
    "    \"\"\" \n",
    "    function: add a set of geograpies to buildings based on 2045 Urbansim model dataset\n",
    "    st: HDF file, urbansim model data\n",
    "    year: year string \n",
    "    weight: column use to evaluate overlaps between target area and sampling area\n",
    "    \"\"\"\n",
    "    ren_dict = {'b_city_id':'CITY', 'county_id':'COUNTY', 'census_bg_id':'BLKGRPID', 'b_zone_id':'TAZ'}\n",
    "    weight = [] if not(weight) else [weight]\n",
    "    dfgeo = pd.merge(st[year + '/buildings'][['parcel_id','b_city_id', 'b_zone_id'] + weight], \n",
    "                        st[year + '/parcels'][['census_bg_id','county_id']], \n",
    "                        left_on='parcel_id', right_index= True, how='left' )\n",
    "    dfgeo.rename(columns = ren_dict, inplace=True)\n",
    "    dfgeo['BLKGRPID'] = dfgeo['COUNTY'] * (10**7) + dfgeo['BLKGRPID']\n",
    "    dfgeo['TRACTID'] =  dfgeo['BLKGRPID'] // 10\n",
    "    dfgeo['REGION'] = 2\n",
    "\n",
    "    return dfgeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# three sampling strategies:\n",
    "# 1. \"LARGEST\": sampling from largest single area (default)\n",
    "# 2. \"OVERLAP\": sampling from largest area and other overlapped area proportions (determinted by HUs and HHs)\n",
    "# 3. \"ALL\": sampling from all areas fully or partially overalapping target area \n",
    "sampling_method = 'ALL'  \n",
    "\n",
    "target_geo = 'TAZ'\n",
    "sample_geo = 'CITY'\n",
    "if target_geo == sample_geo:\n",
    "    sampling_method = 'LARGEST' # overwrite sampling method\n",
    "\n",
    "year = '2045'\n",
    "geo_units = ['COUNTY', 'CITY','TAZ', 'TRACTID', 'BLKGRPID', 'REGION']\n",
    "weight = 'residential_units'\n",
    "prj_name = 'refine'\n",
    "new_id_base = 2 * 10 ** 10 # to add to original id and make sure no potential duplicates in geo, hh or person ids.\n",
    "\n",
    "hdf_in = 'run4032_45.h5'\n",
    "hdf_target = 'run4032_taz_draft_ypsi.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1, make geo cross walk using existing model database"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first test using city as target and sample units\n",
    "# use oakland as test region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sampling_method: ALL\ngeo_cross_walk:  refine_geo_cross_walk.csv\ngeocross: 723    geocross_all: 754\n"
    }
   ],
   "source": [
    "st= pd.HDFStore(hdf_in, 'r')\n",
    "\n",
    "dfgeo = building_geos(st, year, weight)\n",
    "dfgeo = dfgeo.loc[dfgeo.COUNTY == 125] #select Oakland for now\n",
    "dfgeo_grpby = dfgeo.groupby([target_geo, sample_geo])[weight].sum().to_frame(weight).reset_index()\n",
    "\n",
    "geocross_all = dfgeo_grpby.sort_values(by=[target_geo, weight], ascending = False) #sort by the proportion of overlaps\n",
    "geocross = geocross_all.drop_duplicates(target_geo)\n",
    "geocross['SAMPLEGEO'] = geocross[sample_geo]\n",
    "#if sampling method is not on single area, then set 'SAMPLEGEO' to target area id plus new_id_base \n",
    "if sampling_method != 'LARGEST':\n",
    "    geocross_dup = geocross_all.loc[geocross_all.duplicated(target_geo,keep = False)]\n",
    "    geocross.loc[geocross[target_geo].isin(geocross_dup[target_geo].unique()), 'SAMPLEGEO'] = geocross[target_geo] + new_id_base\n",
    "geocross['REGION'] = 2\n",
    "geocross = geocross.astype('int64')\n",
    "\n",
    "geocross.to_csv(prj_name + '_geo_cross_walk.csv')\n",
    "\n",
    "print('sampling_method:', sampling_method)\n",
    "print('geo_cross_walk: ', prj_name + '_geo_cross_walk.csv')\n",
    "print('geocross:', len(geocross), '   geocross_all:', len(geocross_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: make popsim config control table from household control totals"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cats_to_ctrl(dict_cats, target_geo, seed_tbl, dfctrl = None):\n",
    "    \"\"\" Convert UrbanSim HH control total categories to Popsim config control table\n",
    "        dict_cats: category dict\n",
    "        target_geo: refinement geo unit\n",
    "        seed_tbl: value for seed_table column\n",
    "        dfctrl: existing control table \n",
    "    \"\"\"\n",
    "\n",
    "    if dfctrl is None:\n",
    "        dfctrl = pd.DataFrame(columns = ['target', 'geography', 'seed_table', \n",
    "                                            'importance', 'control_field', 'expression'])\n",
    "        indv = 0\n",
    "    else:\n",
    "        indv = len(dfctrl) + 1\n",
    "    \n",
    "    for c in dict_cats.keys():\n",
    "        vname = ('hh' + c) if seed_tbl == 'households' else c\n",
    "        ccount = 0\n",
    "        for vmin, vmax in dict_cats[c]:\n",
    "            indv += 1\n",
    "            ccount += 1\n",
    "            if vmin == vmax:\n",
    "                expression = '({}.{}=={})'.format(seed_tbl, c, str(vmin))\n",
    "            elif vmax == -1:\n",
    "                expression = '({}.{}>={})'.format(seed_tbl, c, str(vmin))\n",
    "            else:\n",
    "                expression = '({}.{}>={}) & ({}.{}<={})'.format(seed_tbl, c, str(vmin), seed_tbl, c, str(vmax))\n",
    "            dfctrl.loc[indv]= [vname + str(vmin), target_geo, seed_tbl, 500, \n",
    "                                    vname.upper() + str(ccount), expression]\n",
    "\n",
    "    return dfctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc= pd.HDFStore(hdf_target, 'r')\n",
    "ctotals = stc['/base/annual_household_control_totals']\n",
    "\n",
    "#convert single value column 'race_id' to 'race_id_min' and 'race_id_min' columns\n",
    "ctotals['race_id_min'] = ctotals['race_id']\n",
    "ctotals['race_id_max'] = ctotals['race_id']\n",
    "\n",
    "cats = [x[:-4] for x in ctotals.columns if 'min' in x]\n",
    "\n",
    "#extract all categories and boundary values from hh control totals\n",
    "dict_cats = {}\n",
    "for c in cats:\n",
    "    dict_cats[c] = ctotals[[c+'_min', c+'_max']].drop_duplicates().sort_values(by=c+'_min').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "config control table:  refine_controls.csv\n"
    }
   ],
   "source": [
    "# build config control table using annual_household_control_totals\n",
    "seed_tbl = 'households'\n",
    "simctrl = cats_to_ctrl(dict_cats, target_geo, seed_tbl)\n",
    "simctrl.loc[len(simctrl) + 1] = ['num_hh', target_geo, seed_tbl, 10000000, 'HHBASE', '(households.persons > 0)']\n",
    "\n",
    "simctrl.to_csv(prj_name + '_controls.csv')\n",
    "print('config control table: ', prj_name + '_controls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "config control table:  refine_controls.csv\n"
    }
   ],
   "source": [
    "# (optional, only when person controls are desired)\n",
    "# add person controls to config controls \n",
    "ptarget_geo = 'TAZ'\n",
    "pseed_tbl = 'persons'\n",
    "dict_pcats = {\n",
    "    'age':[[0, 17], [18, 24], [25, 64], [65, -1]],\n",
    "    'sex':[[1, 1], [2, 2]],\n",
    "    'race_id':[[1, 1], [2, 2], [3, 3], [4,4]]\n",
    "}\n",
    "\n",
    "simctrl = cats_to_ctrl(dict_pcats, ptarget_geo, pseed_tbl, dfctrl= simctrl)\n",
    "simctrl.to_csv(prj_name + '_controls.csv')\n",
    "print('config control table: ', prj_name + '_controls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3. generate geo controls "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special case: already have target database\n",
    "#prepare households and persons tables for summarization purpose(refined/official results)\n",
    "bldg_geos = building_geos(stc, year)\n",
    "\n",
    "households = pd.merge(stc[year + '/households'], bldg_geos[geo_units], \n",
    "                                        left_on='building_id', right_index= True, how='left' )\n",
    "households = households.loc[households.COUNTY == 125]\n",
    "households.dropna(axis=0, inplace=True)\n",
    "households = households.astype('int64')\n",
    "\n",
    "persons = pd.merge(stc[year + '/persons'], households[geo_units], \n",
    "                                        left_on='household_id', right_index= True, how='left' )\n",
    "persons.dropna(axis=0, inplace=True)\n",
    "persons = persons.astype('int64')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "geography control table:  refine_control_totals_taz.csv\n"
    }
   ],
   "source": [
    "#summarize target HH distribution based on config_controls\n",
    "#single geo level for now\n",
    "\n",
    "for ind, r in simctrl.iterrows():\n",
    "    sr = pd.eval('{}.loc[{}]'.format(r.seed_table, r.expression)).groupby([r.geography]).size()\n",
    "    sr.name = r.control_field\n",
    "    if ind == 1:\n",
    "        sumt = sr\n",
    "    else:\n",
    "        sumt = pd.concat([sumt, sr], axis=1)\n",
    "\n",
    "geo_ctrl = pd.DataFrame(sumt)\n",
    "geo_ctrl.index.name = target_geo\n",
    "geo_ctrl['REGION'] = 2\n",
    "geo_ctrl.fillna(0, inplace=True)\n",
    "geo_ctrl = geo_ctrl.astype('int64')\n",
    "\n",
    "geo_ctrl.to_csv('{}_control_totals_{}.csv'.format(prj_name, target_geo.lower()))\n",
    "print('geography control table: ', '{}_control_totals_{}.csv'.format(prj_name, target_geo.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4. make seed HHs and Persons"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sample data\n",
    "bldg_geos = building_geos(st, year)\n",
    "\n",
    "hhs_total = pd.merge(st[year + '/households'], bldg_geos[geo_units], \n",
    "                                    left_on='building_id', right_index= True,how='left') \n",
    "pps_total = pd.merge(st[year + '/persons'], hhs_total[geo_units], \n",
    "                                    left_on='household_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sampling_method ALL\n"
    }
   ],
   "source": [
    "# sample data for sample_geos\n",
    "print ('sampling_method', sampling_method)\n",
    "\n",
    "def sample_largest(df_total, geocross, sample_geo):\n",
    "    dfs = df_total.loc[df_total[sample_geo].isin(geocross[sample_geo].unique())]\n",
    "    dfs['SAMPLEGEO'] = dfs[sample_geo]                      \n",
    "    return dfs.astype('int64')\n",
    "\n",
    "geocross = geocross.loc[geocross['SAMPLEGEO'] <= new_id_base]\n",
    "\n",
    "hhs = sample_largest(hhs_total, geocross, sample_geo)\n",
    "hhs['WGTP'] = 1\n",
    "pps = sample_largest(pps_total, geocross, sample_geo)\n",
    "pps['PWGTP'] = 1\n",
    "\n",
    "if sampling_method == 'LARGEST':\n",
    "    hhs.to_csv(prj_name + '_seed_households.csv')\n",
    "    print('seed households table: ', prj_name + '_seed_households.csv')\n",
    "    pps.to_csv(prj_name + '_seed_persons.csv')\n",
    "    print('seed persons table: ', prj_name + '_seed_persons.csv')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sampling from all overlapping area\n",
    "if sampling_method != 'LARGEST':\n",
    "    hhlst = pplst = []\n",
    "    hhind = ppind = new_id_base\n",
    "\n",
    "    for geo, dfg in geocross_dup.groupby(target_geo):  \n",
    "        if sampling_method == 'OVERLAP':\n",
    "            hh_lc = pd.concat( [hhs_total.loc[hhs_total[sample_geo] == dfg[sample_geo].values[0]], \n",
    "                                hhs_total.loc[(hhs_total[sample_geo].isin(dfg[sample_geo].values[1:])) & \n",
    "                                                (hhs_total[target_geo] == geo)]\n",
    "                                ] )\n",
    "        elif sampling_method == 'ALL':\n",
    "            hh_lc = hhs_total.loc[hhs_total[sample_geo].isin(dfg[sample_geo].values)]\n",
    "        \n",
    "        hh_lc['SAMPLEGEO'] = geo + new_id_base \n",
    "        hh_lc['new_hhid'] = range(hhind, hhind + len(hh_lc))\n",
    "        hhlst.append(hh_lc)\n",
    "        hhind += len(hh_lc)\n",
    "\n",
    "        pp_lc = pps_total.loc[pps_total.household_id.isin(hh_lc.index.values)]\n",
    "        pp_lc = pd.merge(pp_lc, hh_lc[['new_hhid', 'SAMPLEGEO']], \n",
    "                                        left_on = 'household_id', right_index =True, how ='left')\n",
    "        pp_lc.index = range(ppind, ppind + len(pp_lc))\n",
    "        pplst.append(pp_lc)\n",
    "        ppind += len(pp_lc)\n",
    "\n",
    "    dfhhs = pd.concat(hhlst)\n",
    "    dfhhs['WGTP'] = 1\n",
    "    dfhhs.index = dfhhs['new_hhid']\n",
    "    dfhhs.drop('new_hhid', axis=1, inplace=True)\n",
    "\n",
    "    hhs = pd.concat([hhs, dfhhs])\n",
    "    hhs.index.name = 'household_id'\n",
    "    hhs = hhs.astype('int64')\n",
    "\n",
    "    hhs.to_csv(prj_name + '_seed_households.csv')\n",
    "    print('seed households table: ', prj_name + '_seed_households.csv')\n",
    "\n",
    "    dfpps = pd.concat(pplst)\n",
    "    dfpps['PWGTP'] = 1\n",
    "    dfpps['household_id'] = dfpps['new_hhid']\n",
    "    dfpps.drop(['new_hhid'], axis=1, inplace=True)\n",
    "    pps = pd.concat([pps, dfpps])\n",
    "    pps.index.name = 'person_id'\n",
    "    pps = pps.astype('int64')\n",
    "\n",
    "    pps.to_csv(prj_name + '_seed_persons.csv')\n",
    "    print('seed persons table: ', prj_name + '_seed_persons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}