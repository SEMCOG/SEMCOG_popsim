{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import OrderedDict \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read geo info\n",
    "#geo reader\n",
    "def geo_reader(geo_folder, geo_file):\n",
    "        geowidth= [6,2,3,2,3,2,7,1,1,2,3,2,2,5,2,2,5,2,2,6,1,4]\n",
    "        geoflds= ['FILEID','STUSAB','SUMLEV','GEOCOMP','CHARITER', 'CIFSN','LOGRECNO','REGION','DIVISION' ,'STATE' ,'COUNTY' ,\n",
    "                'COUNTYCC' ,'COUNTYSC' ,'COUSUB' ,'COUSUBCC' ,'COUSUBSC' , 'PLACE' ,'PLACECC' ,'PLACESC' ,'TRACT' ,'BLKGRP' ,'BLOCK']\n",
    "                \n",
    "        dfgeo = pd.read_fwf(geo_folder + geo_file, widths =geowidth, names = geoflds, header = None, usecols = range(22))\n",
    "        dfgeo['geoid'] = dfgeo['STATE'] * 10000000000 + dfgeo['COUNTY'] * 10000000 + dfgeo['TRACT'] * 10 + dfgeo['BLKGRP']\n",
    "        return dfgeo[['SUMLEV', 'LOGRECNO','geoid']]\n",
    "\n",
    "# # summary level (https://www.census.gov/prod/cen2010/doc/sf1.pdf)\n",
    "# 050, State-County\n",
    "# 060, State-County-County Subdivision\n",
    "# 067, State-County-County Subdivision-Subminor Civil Division\n",
    "# 140, State-County-Census Tract\n",
    "# 150, State-County-Census Tract-Block Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_folder = '2010_dp/' # dp geo and state files should be in this folder\n",
    "sf1_folder = '2010_sf1/'# sf1 geo and state files should be in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lookup file for DP tables and all variables\n",
    "header_files = {'Segment1_demographicAndHousehold.csv':'mi000012010.dhc',\n",
    "                'Segment2_householdAndGQ.csv':'mi000022010.dhc',\n",
    "                'Segment14_housing.csv':'mi000142010.dhc' }\n",
    "dp_lookup = []\n",
    "for hf in header_files:\n",
    "    segh = pd.read_csv(dp_folder + hf)\n",
    "    segh = segh.columns[5:].to_frame(name = 'FIELD CODE')\n",
    "    segh[\"TABLE\"] = segh['FIELD CODE'].str[:-4]\n",
    "    segh[\"SEGMENT FILE\"] = header_files[hf]\n",
    "    segh[\"SEGMENT\"] = segh[\"SEGMENT FILE\"].str[5:7].astype(int)\n",
    "    segh[\"FIELD NAME\"] = segh[\"FIELD CODE\"]\n",
    "    for seg in segh.SEGMENT:\n",
    "        lseg = len(segh.loc[segh.SEGMENT == seg])\n",
    "        segh.loc[segh.SEGMENT == seg, 'FIELD POS'] =  range(5, lseg + 5)\n",
    "    dp_lookup.append(segh)\n",
    "\n",
    "dp_lookup = pd.concat(dp_lookup, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_lookup['TABLE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get lookup file for SF1 tables and all variables\n",
    "# sf1_lookup = pd.read_excel(sf1_folder + 'DATA_FIELD_DESCRIPTORS.xlsx')\n",
    "# sf1_lookup = sf1_lookup.loc[~sf1_lookup.DECIMAL.isnull()]\n",
    "# sf1_lookup[\"TABLE\"] = sf1_lookup['FIELD CODE'].str[:-4]\n",
    "# sf1_lookup[\"SEGMENT FILE\"] = \"mi000\" + sf1_lookup.SEGMENT.astype(str).str.zfill(2) + \"2010.sf1\"\n",
    "# sf1_lookup['FIELD POS'] = 0\n",
    "# for seg in sf1_lookup.SEGMENT:\n",
    "#     lseg = len(sf1_lookup.loc[sf1_lookup.SEGMENT == seg])\n",
    "#     sf1_lookup.loc[sf1_lookup.SEGMENT == seg, 'FIELD POS'] =  range(5, lseg + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lookup file for sf1 tables and all variables\n",
    "sf1_folder = '2010_sf1_sel/'\n",
    "sf1_files = ['SF1_00001.txt','SF1_00002.txt','SF1_00003.txt','SF1_00004.txt','SF1_00005.txt','SF1_00044.txt']\n",
    "\n",
    "sf1_lookup = []\n",
    "for hf in sf1_files:\n",
    "    segh = pd.read_csv(sf1_folder + hf, index_col=0, nrows=0).columns[4:]\n",
    "    segh = segh.to_frame(name = 'FIELD CODE')\n",
    "    segh[\"TABLE\"] = segh['FIELD CODE'].str[:-4]\n",
    "    segh[\"SEGMENT FILE\"] = hf\n",
    "    segh[\"SEGMENT\"] = segh[\"SEGMENT FILE\"].str[7:8].astype(int)\n",
    "\n",
    "    for seg in segh.SEGMENT:\n",
    "        lseg = len(segh.loc[segh.SEGMENT == seg])\n",
    "        segh.loc[segh.SEGMENT == seg, 'FIELD POS'] =  range(5, lseg + 5)\n",
    "\n",
    "    sf1_lookup.append(segh)\n",
    "sf1_lookup = pd.concat(sf1_lookup)\n",
    "\n",
    "#     segh = segh.columns[5:].to_frame(name = 'FIELD CODE')\n",
    "#     segh[\"TABLE\"] = segh['FIELD CODE'].str[:-4]\n",
    "#     segh[\"SEGMENT FILE\"] = header_files[hf]\n",
    "#     segh[\"SEGMENT\"] = segh[\"SEGMENT FILE\"].str[5:7].astype(int)\n",
    "#     segh[\"FIELD NAME\"] = segh[\"FIELD CODE\"]\n",
    "#     for seg in segh.SEGMENT:\n",
    "#         lseg = len(segh.loc[segh.SEGMENT == seg])\n",
    "#         segh.loc[segh.SEGMENT == seg, 'FIELD POS'] =  range(5, lseg + 5)\n",
    "#     dp_lookup.append(segh)\n",
    "\n",
    "# dp_lookup = pd.concat(dp_lookup, axis=0)\n",
    "\n",
    "\n",
    "name_lookup = pd.read_excel(sf1_folder + 'SelectVariablesToCompare.xlsx')\n",
    "name_lookup = name_lookup.dropna(axis=0)\n",
    "name_lookup = name_lookup[['FIELD CODE', 'FIELD NAME']].set_index('FIELD CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FIELD CODE</th>\n      <th>TABLE</th>\n      <th>SEGMENT FILE</th>\n      <th>SEGMENT</th>\n      <th>FIELD POS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>P0010001</td>\n      <td>P0010001</td>\n      <td>P001</td>\n      <td>SF1_00001.txt</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <td>P0020001</td>\n      <td>P0020001</td>\n      <td>P002</td>\n      <td>SF1_00002.txt</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         FIELD CODE TABLE   SEGMENT FILE  SEGMENT  FIELD POS\nP0010001   P0010001  P001  SF1_00001.txt        0          5\nP0020001   P0020001  P002  SF1_00002.txt        0          5"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf1_lookup.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look up segment number(1), table(P001), or field code(P0020004) and determine segment file, column position and column names(use either fld codes or fld names)\n",
    "def extract_pos_names(lookupf, segment = None, table = None, fields = None):\n",
    "\n",
    "    if (segment is not None) and (segment in list(lookupf.SEGMENT)):\n",
    "        sublk = lookupf.loc[lookupf.SEGMENT\t== segment]\n",
    "\n",
    "    if (table is not None) and (table in list(lookupf.TABLE)):\n",
    "        sublk = lookupf.loc[lookupf.TABLE == table]\n",
    "        #print(sublk)\n",
    "\n",
    "    if (fields is not None) and (fields in list(lookupf['FIELD CODE'])) :\n",
    "        sublk = lookupf.loc[lookupf['FIELD CODE'].isin(fields)]\n",
    "\n",
    "    segfile = sublk['SEGMENT FILE'].unique()[0]\n",
    "    fldpos = list(sublk['FIELD POS'])\n",
    "    fldcodes = list(sublk['FIELD CODE'])\n",
    "    fldnames = list(sublk['FIELD NAME'])\n",
    "    return (segfile, fldpos, fldcodes, fldnames) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Census DP or SF1 segment by variable position \n",
    "def state_file_reader(fpath, fname, vpos = None, vnames = None):\n",
    "    return pd.read_csv(fpath + fname, header = None, usecols = [4] + vpos, names = ['LOGRECNO'] + vnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_data(source, lookupf, table_folder, table_name, sumlevel):\n",
    "    segfile, fldpos, fldcodes, fldnames = extract_pos_names(lookupf, table = table_name )\n",
    "    dpnames = pd.DataFrame(zip(fldcodes, fldnames), columns = ['code','name'])\n",
    "    #fldcodes = [ x + '_' + source for x in fldcodes]\n",
    "    df = state_file_reader(table_folder, segfile, fldpos, fldcodes)\n",
    "    if source == 'dp':\n",
    "        geo_suf = 'dhc'\n",
    "    else:\n",
    "        geo_suf = 'sf1'\n",
    "    geo = geo_reader(table_folder, \"migeo2010.\" + geo_suf)\n",
    "    df = pd.merge(geo, df, left_on = \"LOGRECNO\",right_on = \"LOGRECNO\",  how = 'left', suffixes=(\"\", \"_y\"))\n",
    "    df = df.loc[df.geoid>0]\n",
    "    df = df.loc[df.SUMLEV == sumlevel]\n",
    "    return df, dpnames"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lookup = sf1_lookup[['FIELD CODE', 'FIELD NAME']].set_index('FIELD CODE')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1_lookup.loc[sf1_lookup.TABLE=='P012']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tb in dp_lookup.TABLE.unique():\n",
    "tlist = ['P001', 'P003', 'P004', 'P005', 'P006', 'P007','P013',\n",
    "       'P014', 'P015', 'P016', 'P018', 'P019', 'P020', 'P022', 'P023',\n",
    "       'P024', 'P025', 'P026', 'P028', 'P038', 'P043', 'H0001', 'H003',\n",
    "       'H006', 'H007', 'H010', 'H013']\n",
    "tlist = ['P038', 'P043', 'H0001', 'H003',\n",
    "       'H006', 'H007', 'H010', 'H013']\n",
    "for tb in tlist:\n",
    "    print(tb)\n",
    "    df_dp, dpnames = get_table_data('dp', dp_lookup, dp_folder, tb, 150)\n",
    "    df_sf1, sf1names = get_table_data('sf1', sf1_lookup, sf1_folder, tb, 150)\n",
    "    df_dp.set_index('geoid',inplace=True)\n",
    "    df_sf1.set_index('geoid',inplace=True)\n",
    "\n",
    "    df_diff = df_dp.subtract(df_sf1, axis =1)\n",
    "    df_pct_diff = df_diff.div(df_sf1, axis=1) * 100.0\n",
    "    df_pct_diff.update(df_diff[df_diff==0])\n",
    "    df_pct_diff = df_pct_diff.replace([np.inf, -np.inf],np.nan).dropna(axis=0)\n",
    "    df_pct_diff.drop(columns =['SUMLEV', 'LOGRECNO'], inplace=True)\n",
    "\n",
    "    dfp = dif_summary(df_pct_diff)\n",
    "    dfp.index=name_lookup.loc[dfp.index, 'FIELD NAME']\n",
    "    print('making plot...', tb)\n",
    "    errorbars(dfp, 'plots/', tb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfp)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_summary(df_dif):\n",
    "    dfp = pd.DataFrame(data=[df_dif.mean(), df_dif.std(), df_dif.min(), df_dif.max()]).T\n",
    "    dfp.columns = ['mean', 'std', 'min', 'max']\n",
    "        #plt.gca().invert_yaxis()\n",
    "    dfp = dfp.sort_index(axis = 0, ascending = False)\n",
    "    return dfp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorbars(df, plot_folder, plot_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    sns.set(font_scale=1)\n",
    "    #plt.figure(figsize = (10, max(5, len(df)/1.5))\n",
    "    plt.figure(figsize = (10, len(df))\n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "    plt.vlines(0, 0, len(df))\n",
    "    plt.errorbar(df['mean'], range(len(df.index)), xerr= [df['mean'] - df['min'], df['max'] - df['mean']],\n",
    "                fmt='ok', ecolor='gray', lw=1, capsize=8)\n",
    "    plt.errorbar(df['mean'], range(len(df.index)), xerr=df['std'], fmt='ok', lw=4)\n",
    "    for (x,y,l) in zip(df['mean'],  range(len(df.index)), round(df['mean'],3)):\n",
    "        plt.text(3, y + 0.1, l, size=12)\n",
    "    plt.xticks()\n",
    "    plt.yticks(range(len(df.index)), list(df.index))\n",
    "    plt.title(plot_name + ' PCT Block Group Errors')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(plot_folder + plot_name + '_error.png')\n",
    "    print('save plot to ' +plot_folder + plot_name + '_error.png')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pct_diff)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_diff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_diff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_diff.loc[df_diff.P0030006==0,'P0030006' ] =0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_diff"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1names.set_index('code', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.merge(df_sf1, df_dp, left_on=\"geoid\", right_on=\"geoid\", how=\"left\", suffixes=(\"\", \"_y\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1names"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbln = \"P003\"\n",
    "segfile, fldpos, fldcodes, fldnames = extract_pos_names(dp_lookup, table = tbln )\n",
    "dpnamelookup = pd.DataFrame(zip(fldcodes, fldnames), columns = ['code','name'])\n",
    "fldcodes = [ x + '_dp' for x in fldcodes]\n",
    "df_dp = state_file_reader(dp_folder, segfile, fldpos, fldcodes )\n",
    "geo_dp = geo_reader(dp_folder, \"migeo2010.dhc\")\n",
    "df_dp = pd.merge(geo_dp, df_dp, left_on = \"LOGRECNO\",right_on = \"LOGRECNO\",  how = 'left', suffixes=(\"\", \"_y\"))\n",
    "df_dp = df_dp.loc[df_dp.geoid>0]\n",
    "df_dp = df_dp.loc[df_dp.SUMLEV == 150]\n",
    "\n",
    "segfile, fldpos, fldcodes, fldnames = extract_pos_names(sf1_lookup, table = tbln )\n",
    "sf1namelookup = pd.DataFrame(zip(fldcodes, fldnames), columns = ['code','name'])\n",
    "fldcodes = [ x + '_sf1' for x in fldcodes]\n",
    "df_sf1 = state_file_reader(sf1_folder, segfile, fldpos, fldcodes )\n",
    "geo_sf1 = geo_reader(sf1_folder, \"migeo2010.sf1\" )\n",
    "df_sf1 = pd.merge(geo_sf1, df_sf1, left_on = \"LOGRECNO\",right_on = \"LOGRECNO\",  how = 'left', suffixes=(\"\", \"_y\"))\n",
    "df_sf1 = df_sf1.loc[df_sf1.geoid>0]\n",
    "df_sf1 = df_sf1.loc[df_sf1.SUMLEV == 150]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sf1), len(df_dp)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.merge(df_sf1, df_dp, left_on=\"geoid\", right_on=\"geoid\", how=\"left\", suffixes=(\"\", \"_y\"))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "flds_sf1 = [x.replace('_sf1', '') for x in df_comp.columns.values if x.find('_sf1') >=0 ]\n",
    "flds_dp = [x.replace('_dp', '') for x in df_comp.columns.values if x.find('_dp') >=0 ]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diffs = [] \n",
    "for fld in set(flds_sf1 + flds_dp):\n",
    "    pct_diffs.append(fld + '_pct_diff')\n",
    "    df_comp[fld + '_pct_diff'] = (df_comp[fld + '_dp'] - df_comp[fld + '_sf1'])/df_comp[fld + '_sf1'] * 100.0\n",
    "    df_comp.loc[df_comp[fld + '_sf1'] == 0, fld + '_pct_diff'] = 300\n",
    "    df_comp.loc[(df_comp[fld + '_dp'] - df_comp[fld + '_sf1']) == 0, fld + '_pct_diff'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diffs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df_comp.copy()\n",
    "dfs = dfs.replace([np.inf, -np.inf], np.nan)\n",
    "dfs.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.T"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = dfp.sort_index(axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorbars(df, chartname):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize = (20,30))\n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "\n",
    "    plt.vlines(0, 0, len(df))\n",
    "    plt.errorbar(df['mean'], range(len(df.index)), xerr= [df['mean'] - df['min'], df['max'] - df['mean']],\n",
    "                fmt='ok', ecolor='gray', lw=2, capsize=8)\n",
    "    plt.errorbar(df['mean'], range(len(df.index)), xerr=df['std'], fmt='ok', lw=4)\n",
    "    for (x,y,l) in zip(df['mean'],  range(len(df.index)), round(df['mean'],3)):\n",
    "        plt.text(3, y + 0.1, l, size=16)\n",
    "    plt.xticks()\n",
    "    plt.yticks(range(len(df.index)), list(df.name))\n",
    "    plt.title(chartname + ' Block Group Errors')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorbars(dfp,'P003')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.COUNTY"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.COUNTY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3] .+ 5"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_reader(folder, segfile, vars):\n",
    "    print(folder, segfile, vars)\n",
    "    return pd.read_csv(folder + segfile, header = 0, usecols=vars.values)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in dp_tables.TABLE.unique():\n",
    "    dp_seg = dp_tables.loc[dp_tables.TABLE == t, \"SEGMENT\"]\n",
    "    dp_vars = dp_tables.loc[dp_tables.TABLE == t, \"FIELD CODE\"]\n",
    "    #print(t, dp_seg, dp_vars)\n",
    "    dp_reader('2010_dp/', dp_seg.unique()[0], dp_vars)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_seg.unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('2010_dp/'+dp_seg.unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get header from DP data\n",
    "for h in header_files:\n",
    "    seg_header = pd.read_csv(folder + h)\n",
    "    \n",
    "\n",
    "#get unique table number from dp\n",
    "df_dp = dfseg\n",
    "df_table = df_dp.columns[5:].to_frame(name = 'field')\n",
    "df_table['tablen'] = df_table['field'].str[:-4]\n",
    "tn = df_table['tablen'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp.columns.values[5:]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfseg.columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf1_seg_reader(folder, segnum, dffld):\n",
    "    geo_header = ['FILEID','STUSAB','CHARITER','CIFSN','LOGRECNO']\n",
    "    fn = \"mi000\" + str(segnum).zfill(2) + \"2010.sf1\" \n",
    "    header = geo_header + list(dffld.loc[dffld['SEGMENT'] == segnum, 'FIELD CODE'].values)\n",
    "    #print(header)\n",
    "    dfdata = pd.read_csv(folder + fn, header = None, names = header )\n",
    "\n",
    "    return dfdata  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '2010_sf1/'\n",
    "df_sf1 = pd.read_fwf(folder + \"migeo2010.sf1\", widths =geowidth, names = geoflds , header = None, usecols = range(22))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffld = pd.read_excel('2010_sf1/DATA_FIELD_DESCRIPTORS.xlsx')\n",
    "dffld = dffld.loc[~dffld.DECIMAL.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_sf1_seg = dffld.loc[dffld['FIELD CODE'].isin(dfseg.columns.values ),'SEGMENT'].unique()\n",
    "print(dp_sf1_seg)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_header = ['FILEID','STUSAB','CHARITER','CIFSN','LOGRECNO']\n",
    "\n",
    "for x in dp_sf1_seg:\n",
    "    dpseg = sf1_seg_reader(folder, x, dffld)\n",
    "    df_sf1 = pd.merge(df_sf1, dpseg, left_on =geo_header, right_on = geo_header, how ='left' )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf1.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumlevel = {'COUNTY': 50, 'TRACT': 140, 'Block Group': 150}\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfbg = df_sf1.loc(df_sf1.SUMLEV = sumlevel['BLKGRP'])\n",
    "df_sf1.loc(df_sf1.SUMLEV = sumlevel['BLKGRP'])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf1_seg_reader(folder, segnum, dffld):\n",
    "    geo_header = ['FILEID','STUSAB','CHARITER','CIFSN','LOGRECNO']\n",
    "    fn = \"mi000\" + str(segnum).zfill(2) + \"2010.sf1\" \n",
    "    header = geo_header + list(dffld.loc[dffld['SEGMENT'] == segnum, 'FIELD CODE'].values)\n",
    "    #print(header)\n",
    "    dfdata = pd.read_csv(folder + fn, header = None, names = header )\n",
    "\n",
    "    return dfdata   \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1_seg_reader('2010_sf1/', 1, dffld)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(2).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfseg.columns)[5]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsec1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgeo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfgeo), len(dfsec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm  = pd.merge(dfgeo, dfsec1, left_on ='LOGRECNO', right_on = 'LOGRECNO', how ='left', suffixes = (\"\", \"_y\") )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wide_160/wide_dp1_codebook.txt\", \"r\")\n",
    "dict_v = OrderedDict()\n",
    "dict_v = {'FILEID':'FILEID','STUSAB':'STUSAB','CHARITER':'CHARITER','CIFSN':'CIFSN','LOGRECNO':'LOGRECNO'}\n",
    "geo_remove = list(dict_v.keys())[:4]\n",
    "for x in f:\n",
    "    #print(x)\n",
    "    if bool(re.search(\"H.....:\", x)):\n",
    "        v = x.split(\":\")[0].strip()\n",
    "        vname = v[:3] + \"_\" + x.replace(v + \":\", '').strip()\n",
    "        dict_v[v] = vname\n",
    "        #print (v, ',', vname)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read differential privacy 2010 demo data\n",
    "dfsec1 = pd.read_csv(\"mi000012010.dhc\", header = None, names = list(dict_v.keys()) )\n",
    "dfsec1.drop(columns=geo_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoflds =  ['FILEID','STUSAB','SUMLEV','GEOCOMP','CHARITER','CIFSN','LOGRECNO']\n",
    "# wlst = [6,2,3,2,3,2,7]\n",
    "# wlst2= [6,2,3,2,3,2,7,1,1,2,3,2,2,5,2,2,5,2,2,6,1,4]\n",
    "# geoflds2 = ['LOGRECNO','REGION','DIVISION' ,'STATE' ,'COUNTY' ,'COUNTYCC' ,'COUNTYSC' ,'COUSUB' ,'COUSUBCC' ,'COUSUBSC' ,'PLACE' ,'PLACECC' ,'PLACESC' ,'TRACT' ,'BLKGRP' ,'BLOCK']\n",
    "# colss = [(18, 25), (25, 26), (26, 27), (27, 29), (29, 32), (32, 34), (34, 36), (36, 41), (41, 43), (43, 45), (45, 50), (50, 52), (52, 54), (54, 60), (60, 61), (61, 65)]\n",
    "\n",
    "geowidth= [6,2,3,2,3,2,7,1,1,2,3,2,2,5,2,2,5,2,2,6,1,4]\n",
    "geoflds= ['FILEID','STUSAB','SUMLEV','GEOCOMP','CHARITER','CIFSN','LOGRECNO','REGION','DIVISION' ,'STATE' ,'COUNTY' ,'COUNTYCC' ,'COUNTYSC' ,'COUSUB' ,'COUSUBCC' ,'COUSUBSC' ,'PLACE' ,'PLACECC' ,'PLACESC' ,'TRACT' ,'BLKGRP' ,'BLOCK']\n",
    "keepflds = [ 'SUMLEV', 'GEOCOMP',\n",
    "       'LOGRECNO', 'REGION', 'DIVISION', 'STATE', 'COUNTY', 'COUNTYCC',\n",
    "       'COUNTYSC', 'COUSUB', 'COUSUBCC', 'COUSUBSC', 'PLACE', 'PLACECC',\n",
    "       'PLACESC', 'TRACT', 'BLKGRP', 'BLOCK']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgeo = pd.read_fwf(\"migeo2010.dhc\", widths =geowidth, names = geoflds, header = None, usecols = range(22))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgeo.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfgeo), len(dfsec1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm  = pd.merge(dfgeo, dfsec1, left_on ='LOGRECNO', right_on = 'LOGRECNO', how ='left' )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_v.keys())[4:] + "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfm.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols = list(dfgeo.columns.values) + list(dict_v.values())[5:]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.loc[dfm.SUMLEV == 91].head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.columns = newcols"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * process 2010 SF1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgeosf = pd.read_fwf(\"2010_sf1/migeo2010.sf1\", widths =geowidth, names = geoflds , header = None, usecols = range(22))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgeosf.BLKGRP.unique()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffld = pd.read_excel('2010_sf1/DATA_FIELD_DESCRIPTORS.xlsx')\n",
    "dffld = dffld.loc[~dffld.DECIMAL.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffld"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_header = ['FILEID','STUSAB','CHARITER','CIFSN','LOGRECNO']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffld.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'FILEID':'FILEID','STUSAB':'STUSAB','CHARITER':'CHARITER','CIFSN':'CIFSN','LOGRECNO':'LOGRECNO'}\n",
    "\n",
    "seg_tables = list(dffld.loc[dffld.SEGMENT == 2]['TABLE NUMBER'].values)\n",
    "seg_header = list(dffld.loc[dffld.SEGMENT == 2]['FIELD NAME'].values)\n",
    "seg_header = [ x + y for x, y in zip(seg_tables, seg_header)]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_header + seg_header"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "seglist = [1,2,3]\n",
    "df_segs = []\n",
    "for seg in seglist:\n",
    "    print('read segment', seg)\n",
    "    seg_tables = list(dffld.loc[dffld.SEGMENT == seg]['FIELD CODE'].values)\n",
    "    seg_header = list(dffld.loc[dffld.SEGMENT == seg]['FIELD NAME'].values)\n",
    "    seg_header = [ x + y for x, y in zip(seg_tables, seg_header)]\n",
    "\n",
    "    df_seg = pd.read_csv(\"2010_sf1/\" + \"mi\" + str(seg).zfill(5) + '2010.sf1', header = None, names = geo_header + seg_header)\n",
    "    df_segs.append(df_seg) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_segs)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfgeosf),"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = \n",
    "#DHCST MI420000000000000012326\n",
    "#DHCST MI420000000000000022326"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_geo = OrderedDict()\n",
    "dict_geo = {}\n",
    "State/U.S. Abbreviation (USPS) . . . . . . . . . . \tSTUSAB \n",
    "Summary Level . . . . . . . . . . . . . . . . . . . . . . \tSUMLEV \n",
    "Geographic Component . . . . . . . . . . . . . . . \tGEOCOMP \n",
    "Characteristic Iteration . . . . . . . . . . . . . . . . . \tCHARITER \n",
    "Characteristic Iteration File Sequence\t\n",
    "Number . . . . . . . . . . . . . . . . . . . . . . . . . . . \tCIFSN \n",
    "Logical Record Number . . . . . . . . . . . . . . . . \tLOGRECNO \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wide_160/wide_dp1_160.csv\", encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "difp = []\n",
    "for f in flist:\n",
    "    df[f + '_dif'] = df[f + '_dp'] - df[f + '_sf']\n",
    "    df[f + '_difp'] = df[f + '_dif']/df[f + '_sf']\n",
    "    diff.append(f + '_dif')\n",
    "    difp.append(f+'_difp')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[difp]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gisjoin"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x = 'H7X002_sf', y= 'H7X002_dp')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['H7V001_difp'](),.mean df['H7V001_difp'].std()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =df['H7V001_difp'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}